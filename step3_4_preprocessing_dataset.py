"""
B∆∞·ªõc 3 & 4: Ti·ªÅn x·ª≠ l√Ω D·ªØ li·ªáu v√† T·∫°o Dataset Hu·∫•n luy·ªán
M·ª•c ti√™u: Chu·∫©n b·ªã d·ªØ li·ªáu s·∫°ch v√† t·∫°o 3D patches cho Deep Learning
"""

import numpy as np
import matplotlib.pyplot as plt
import spectral.io.envi as envi
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from scipy.signal import savgol_filter
import pickle
import time
from tqdm import tqdm

# C·∫•u h√¨nh
np.random.seed(42)  # ƒê·∫£m b·∫£o reproducibility

def load_data():
    """
    T·∫£i d·ªØ li·ªáu HSI v√† Ground Truth (t√°i s·ª≠ d·ª•ng t·ª´ B∆∞·ªõc 2)
    """
    print("=" * 60)
    print("üîÑ B∆Ø·ªöC 3 & 4: TI·ªÄN X·ª¨ L√ù V√Ä T·∫°O DATASET")
    print("=" * 60)
    
    data_path = r"d:\HyperSpectral_Imaging_Crop_Analysis\Eggplant_Crop\Eggplant_Crop"
    
    # File paths
    hsi_header = os.path.join(data_path, "Eggplant_Reflectance_Data.hdr")
    hsi_data = os.path.join(data_path, "Eggplant_Reflectance_Data")
    gt_header = os.path.join(data_path, "Eggplant_N2_Concentration_GT.hdr")
    gt_data = os.path.join(data_path, "Eggplant_N2_Concentration_GT")
    
    try:
        print("üîÑ ƒêang t·∫£i l·∫°i d·ªØ li·ªáu...")
        hsi_img = envi.open(hsi_header, hsi_data)
        hsi_array = hsi_img.load()
        
        gt_img = envi.open(gt_header, gt_data)
        gt_array = gt_img.load()
        
        # X·ª≠ l√Ω GT array shape
        if len(gt_array.shape) == 3 and gt_array.shape[2] == 1:
            gt_array = gt_array[:, :, 0]
        
        gt_array = gt_array.astype(int)
        
        print(f"‚úÖ HSI Shape: {hsi_array.shape}")
        print(f"‚úÖ GT Shape: {gt_array.shape}")
        
        return hsi_array, gt_array, hsi_img
        
    except Exception as e:
        print(f"‚ùå L·ªói khi t·∫£i d·ªØ li·ªáu: {str(e)}")
        return None, None, None

def step3_preprocessing(hsi_array, apply_smoothing=False):
    """
    B∆∞·ªõc 3: Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu HSI
    """
    print("\n" + "=" * 60)
    print("üßπ B∆Ø·ªöC 3: TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU")
    print("=" * 60)
    
    print(f"üìä D·ªØ li·ªáu g·ªëc - Min: {np.min(hsi_array):.4f}, Max: {np.max(hsi_array):.4f}")
    
    # 3.1: Savitzky-Golay Smoothing (t√πy ch·ªçn)
    if apply_smoothing:
        print("üîÑ √Åp d·ª•ng Savitzky-Golay smoothing...")
        hsi_smoothed = np.zeros_like(hsi_array)
        
        for i in tqdm(range(hsi_array.shape[0]), desc="Smoothing rows"):
            for j in range(hsi_array.shape[1]):
                spectrum = hsi_array[i, j, :]
                # √Åp d·ª•ng SG filter v·ªõi window=5, polynomial=2
                try:
                    smoothed_spectrum = savgol_filter(spectrum, window_length=5, polyorder=2)
                    hsi_smoothed[i, j, :] = smoothed_spectrum
                except:
                    hsi_smoothed[i, j, :] = spectrum  # Gi·ªØ nguy√™n n·∫øu l·ªói
        
        hsi_array = hsi_smoothed
        print("‚úÖ Smoothing ho√†n th√†nh!")
    else:
        print("‚è≠Ô∏è B·ªè qua Savitzky-Golay smoothing (d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l·ªçc)")
    
    # 3.2: Chu·∫©n h√≥a d·ªØ li·ªáu v·ªÅ [0, 1]
    print("üîÑ Chu·∫©n h√≥a d·ªØ li·ªáu v·ªÅ [0, 1]...")
    
    # T√¨m min/max to√†n c·ª•c
    global_min = float(np.min(hsi_array))
    global_max = float(np.max(hsi_array))
    
    print(f"   Global Min: {global_min:.4f}")
    print(f"   Global Max: {global_max:.4f}")
    
    # Chu·∫©n h√≥a
    hsi_normalized = (hsi_array - global_min) / (global_max - global_min)
    hsi_normalized = np.clip(hsi_normalized, 0, 1)
    
    print(f"‚úÖ Sau chu·∫©n h√≥a - Min: {np.min(hsi_normalized):.4f}, Max: {np.max(hsi_normalized):.4f}")
    
    return hsi_normalized, global_min, global_max

def step3_label_processing(gt_array):
    """
    B∆∞·ªõc 3: X·ª≠ l√Ω nh√£n - chuy·ªÉn t·ª´ (1,2,3) th√†nh (0,1,2)
    """
    print("\nüè∑Ô∏è X·ª¨ L√ù NH√ÉN:")
    
    # Hi·ªÉn th·ªã ph√¢n b·ªë g·ªëc
    unique_labels, counts = np.unique(gt_array, return_counts=True)
    print("üìä Ph√¢n b·ªë nh√£n g·ªëc:")
    class_names_old = ['Unclassified', 'Low N2', 'Medium N2', 'High N2']
    for label, count in zip(unique_labels, counts):
        if label < len(class_names_old):
            print(f"   {class_names_old[label]} (L·ªõp {label}): {count:,} pixels")
    
    # T·∫°o nh√£n m·ªõi cho Deep Learning
    gt_processed = gt_array.copy()
    
    # Chuy·ªÉn ƒë·ªïi: 1‚Üí1, 2‚Üí2, 3‚Üí3, gi·ªØ nguy√™n 0 (Unclassified)
    # Sau ƒë√≥ s·∫Ω tr·ª´ 1 cho c√°c class training ƒë·ªÉ c√≥ 0,1,2
    # Kh√¥ng c·∫ßn √°nh x·∫° ·ªü ƒë√¢y
    
    # Hi·ªÉn th·ªã k·∫øt qu·∫£
    print("\nüìã √Ånh x·∫° nh√£n cho Deep Learning:")
    print("   L·ªõp 1 (Low N2)    ‚Üí Nh√£n 0 (sau khi tr·ª´ 1)")
    print("   L·ªõp 2 (Medium N2) ‚Üí Nh√£n 1 (sau khi tr·ª´ 1)") 
    print("   L·ªõp 3 (High N2)   ‚Üí Nh√£n 2 (sau khi tr·ª´ 1)")
    print("   L·ªõp 0 (Unclassified) ‚Üí Lo·∫°i b·ªè kh·ªèi training")
    
    # Th·ªëng k√™ nh√£n m·ªõi (ch·ªâ c√°c class c√≥ th·ªÉ train ƒë∆∞·ª£c)
    training_mask = gt_processed > 0  # Lo·∫°i b·ªè Unclassified (0)
    
    # Hi·ªÉn th·ªã ph√¢n b·ªë sau khi t·∫°o training mask
    training_labels = gt_processed[training_mask] - 1  # Chuy·ªÉn v·ªÅ 0,1,2
    unique_mapped, counts_mapped = np.unique(training_labels, return_counts=True)
    print(f"\nüìà Ph√¢n b·ªë nh√£n training (sau khi tr·ª´ 1):")
    class_names_mapped = {0: 'Low N2', 1: 'Medium N2', 2: 'High N2'}
    for label, count in zip(unique_mapped, counts_mapped):
        if label in class_names_mapped:
            print(f"   {class_names_mapped[label]} (Nh√£n {label}): {count:,} pixels")
    
    total_training = np.sum(counts_mapped)
    print(f"üìä T·ªïng s·ªë pixels training: {total_training:,}")
    
    return gt_processed, training_mask

def step4_extract_patches(hsi_normalized, gt_processed, training_mask, patch_size=9, max_samples_per_class=50000):
    """
    B∆∞·ªõc 4: Tr√≠ch xu·∫•t 3D patches t·ª´ d·ªØ li·ªáu HSI
    """
    print("\n" + "=" * 60)
    print("üéØ B∆Ø·ªöC 4: TR√çCH XU·∫§T 3D PATCHES")
    print("=" * 60)
    
    print(f"üìê Patch size: {patch_size}x{patch_size}x{hsi_normalized.shape[2]}")
    print(f"üé≤ Max samples per class: {max_samples_per_class:,}")
    
    height, width, n_bands = hsi_normalized.shape
    half_patch = patch_size // 2
    
    patches = []
    labels = []
    
    # T·∫°o class mapping cho balanced sampling
    class_pixels = {}
    for class_id in range(3):  # 0: Low, 1: Medium, 2: High
        # S·ª≠ d·ª•ng nh√£n g·ªëc (1,2,3) ƒë·ªÉ t√¨m pixels, sau ƒë√≥ √°nh x·∫° th√†nh (0,1,2)
        original_class_id = class_id + 1  # 0‚Üí1, 1‚Üí2, 2‚Üí3
        mask = (gt_processed == original_class_id) & training_mask
        indices = np.where(mask)
        class_pixels[class_id] = list(zip(indices[0], indices[1]))
        print(f"üîç Class {class_id} ({['Low', 'Medium', 'High'][class_id]} N2): {len(class_pixels[class_id]):,} pixels")
    
    # Balanced sampling t·ª´ m·ªói class
    for class_id in range(3):
        pixels = class_pixels[class_id]
        
        # Random sample n·∫øu c√≥ qu√° nhi·ªÅu pixels
        if len(pixels) > max_samples_per_class:
            pixels = np.random.choice(len(pixels), max_samples_per_class, replace=False)
            pixels = [class_pixels[class_id][i] for i in pixels]
            print(f"   üìä Downsampled to {len(pixels):,} pixels")
        
        class_patches = []
        valid_patches = 0
        
        for i, j in tqdm(pixels, desc=f"Extracting Class {class_id} patches"):
            # Ki·ªÉm tra boundaries
            if (i >= half_patch and i < height - half_patch and 
                j >= half_patch and j < width - half_patch):
                
                # Tr√≠ch xu·∫•t patch 3D
                patch = hsi_normalized[i-half_patch:i+half_patch+1, 
                                     j-half_patch:j+half_patch+1, :]
                
                # Ki·ªÉm tra patch shape
                if patch.shape == (patch_size, patch_size, n_bands):
                    class_patches.append(patch)
                    valid_patches += 1
        
        # Th√™m v√†o dataset
        patches.extend(class_patches)
        labels.extend([class_id] * len(class_patches))
        
        print(f"‚úÖ Class {class_id}: {len(class_patches):,} patches extracted")
    
    # Convert to numpy arrays
    patches = np.array(patches, dtype=np.float32)
    labels = np.array(labels, dtype=np.int32)
    
    print(f"\nüìä DATASET SUMMARY:")
    print(f"   üìê Patches shape: {patches.shape}")
    print(f"   üè∑Ô∏è Labels shape: {labels.shape}")
    print(f"   üíæ Memory usage: {patches.nbytes / (1024**2):.1f} MB")
    
    # Th·ªëng k√™ ph√¢n b·ªë final
    unique_final, counts_final = np.unique(labels, return_counts=True)
    for label, count in zip(unique_final, counts_final):
        print(f"   Class {label}: {count:,} patches ({count/len(labels)*100:.1f}%)")
    
    return patches, labels

def step4_train_val_test_split(patches, labels, test_size=0.15, val_size=0.15):
    """
    B∆∞·ªõc 4: Chia dataset th√†nh train/validation/test
    """
    print("\n" + "=" * 60)
    print("üîÑ CHIA DATASET TRAIN/VAL/TEST")
    print("=" * 60)
    
    # T√≠nh to√°n t·ª∑ l·ªá
    train_size = 1.0 - test_size - val_size
    print(f"üìä T·ª∑ l·ªá chia: Train {train_size:.0%} / Val {val_size:.0%} / Test {test_size:.0%}")
    
    # Chia train vs (val+test) tr∆∞·ªõc
    X_train, X_temp, y_train, y_temp = train_test_split(
        patches, labels, 
        test_size=(test_size + val_size),
        stratify=labels,
        random_state=42
    )
    
    # Chia val vs test
    relative_val_size = val_size / (test_size + val_size)
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp,
        test_size=(1 - relative_val_size),
        stratify=y_temp,
        random_state=42
    )
    
    # Th·ªëng k√™ k·∫øt qu·∫£
    print(f"\nüìà K·∫æT QU·∫¢ CHIA DATASET:")
    print(f"   üèãÔ∏è Training:   {X_train.shape[0]:,} samples ({X_train.shape[0]/len(patches)*100:.1f}%)")
    print(f"   ‚úÖ Validation: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(patches)*100:.1f}%)")
    print(f"   üß™ Testing:    {X_test.shape[0]:,} samples ({X_test.shape[0]/len(patches)*100:.1f}%)")
    
    # Ki·ªÉm tra stratification
    print(f"\nüéØ KI·ªÇM TRA STRATIFICATION:")
    datasets = [('Train', y_train), ('Val', y_val), ('Test', y_test)]
    
    for name, y_data in datasets:
        unique_labels, counts = np.unique(y_data, return_counts=True)
        percentages = counts / len(y_data) * 100
        print(f"   {name:10}: ", end="")
        for label, pct in zip(unique_labels, percentages):
            print(f"Class {label}: {pct:.1f}%  ", end="")
        print()
    
    return X_train, X_val, X_test, y_train, y_val, y_test

def save_dataset(X_train, X_val, X_test, y_train, y_val, y_test, 
                 global_min, global_max, save_dir="processed_data"):
    """
    L∆∞u dataset ƒë√£ x·ª≠ l√Ω
    """
    print("\n" + "=" * 60)
    print("üíæ L∆ØU DATASET")
    print("=" * 60)
    
    # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥, x√≥a d·ªØ li·ªáu c≈© n·∫øu c√≥
    if os.path.exists(save_dir):
        print(f"üóëÔ∏è X√≥a d·ªØ li·ªáu c≈© trong th∆∞ m·ª•c: {save_dir}")
        import shutil
        shutil.rmtree(save_dir)
    
    os.makedirs(save_dir)
    print(f"üìÅ T·∫°o th∆∞ m·ª•c m·ªõi: {save_dir}")
    
    # L∆∞u c√°c file
    datasets = {
        'X_train.npy': X_train,
        'X_val.npy': X_val, 
        'X_test.npy': X_test,
        'y_train.npy': y_train,
        'y_val.npy': y_val,
        'y_test.npy': y_test
    }
    
    total_size = 0
    for filename, data in datasets.items():
        filepath = os.path.join(save_dir, filename)
        np.save(filepath, data)
        size_mb = os.path.getsize(filepath) / (1024**2)
        total_size += size_mb
        print(f"‚úÖ Saved {filename:12} - {data.shape} - {size_mb:.1f} MB")
    
    # L∆∞u metadata
    metadata = {
        'patch_size': X_train.shape[1],  # Assuming square patches
        'n_bands': X_train.shape[3],
        'n_classes': len(np.unique(y_train)),
        'global_min': global_min,
        'global_max': global_max,
        'class_names': ['Low N2', 'Medium N2', 'High N2'],
        'total_samples': len(X_train) + len(X_val) + len(X_test)
    }
    
    metadata_path = os.path.join(save_dir, 'metadata.pkl')
    with open(metadata_path, 'wb') as f:
        pickle.dump(metadata, f)
    
    print(f"‚úÖ Saved metadata.pkl")
    print(f"üíæ Total dataset size: {total_size:.1f} MB")
    print(f"üìÅ Saved in: {os.path.abspath(save_dir)}")

def main():
    """
    H√†m ch√≠nh th·ª±c hi·ªán to√†n b·ªô quy tr√¨nh B∆∞·ªõc 3 & 4
    """
    start_time = time.time()
    
    # Load data
    hsi_array, gt_array, hsi_img = load_data()
    if hsi_array is None:
        return
    
    # B∆∞·ªõc 3.1: Preprocessing HSI
    hsi_normalized, global_min, global_max = step3_preprocessing(
        hsi_array, apply_smoothing=False
    )
    
    # B∆∞·ªõc 3.2: Label processing  
    gt_processed, training_mask = step3_label_processing(gt_array)
    
    # B∆∞·ªõc 4.1: Extract 3D patches
    patches, labels = step4_extract_patches(
        hsi_normalized, gt_processed, training_mask, 
        patch_size=9, max_samples_per_class=50000
    )
    
    # B∆∞·ªõc 4.2: Train/Val/Test split
    X_train, X_val, X_test, y_train, y_val, y_test = step4_train_val_test_split(
        patches, labels, test_size=0.15, val_size=0.15
    )
    
    # B∆∞·ªõc 4.3: Save dataset
    save_dataset(X_train, X_val, X_test, y_train, y_val, y_test,
                 global_min, global_max)
    
    # Summary
    elapsed_time = time.time() - start_time
    print("\n" + "=" * 60)
    print("üéâ HO√ÄN TH√ÄNH B∆Ø·ªöC 3 & 4")
    print("=" * 60)
    print("‚úÖ Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu ho√†n th√†nh")
    print("‚úÖ 3D patches ƒë√£ ƒë∆∞·ª£c t·∫°o")
    print("‚úÖ Dataset ƒë√£ ƒë∆∞·ª£c chia train/val/test")
    print("‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u")
    print(f"‚è±Ô∏è Th·ªùi gian th·ª±c hi·ªán: {elapsed_time/60:.1f} ph√∫t")
    print("‚û°Ô∏è S·∫µn s√†ng cho B∆∞·ªõc 5: X√¢y d·ª±ng 3D-ResNet")
    print("=" * 60)

if __name__ == "__main__":
    main()
