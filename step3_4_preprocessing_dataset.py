"""
BÆ°á»›c 3 & 4: Tiá»n xá»­ lÃ½ Dá»¯ liá»‡u vÃ  Táº¡o Dataset Huáº¥n luyá»‡n
Má»¥c tiÃªu: Chuáº©n bá»‹ dá»¯ liá»‡u sáº¡ch vÃ  táº¡o 3D patches cho Deep Learning
"""

import numpy as np
import matplotlib.pyplot as plt
import spectral.io.envi as envi
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from scipy.signal import savgol_filter
import pickle
import time
from tqdm import tqdm

# Cáº¥u hÃ¬nh
np.random.seed(42)  # Äáº£m báº£o reproducibility

def load_data():
    """
    Táº£i dá»¯ liá»‡u HSI vÃ  Ground Truth (tÃ¡i sá»­ dá»¥ng tá»« BÆ°á»›c 2)
    """
    print("=" * 60)
    print("ğŸ”„ BÆ¯á»šC 3 & 4: TIá»€N Xá»¬ LÃ VÃ€ Táº¡O DATASET")
    print("=" * 60)
    
    data_path = r"d:\HyperSpectral_Imaging_Crop_Analysis\Eggplant_Crop\Eggplant_Crop"
    
    # File paths
    hsi_header = os.path.join(data_path, "Eggplant_Reflectance_Data.hdr")
    hsi_data = os.path.join(data_path, "Eggplant_Reflectance_Data")
    gt_header = os.path.join(data_path, "Eggplant_N2_Concentration_GT.hdr")
    gt_data = os.path.join(data_path, "Eggplant_N2_Concentration_GT")
    
    try:
        print("ğŸ”„ Äang táº£i láº¡i dá»¯ liá»‡u...")
        hsi_img = envi.open(hsi_header, hsi_data)
        hsi_array = hsi_img.load()
        
        gt_img = envi.open(gt_header, gt_data)
        gt_array = gt_img.load()
        
        # Xá»­ lÃ½ GT array shape
        if len(gt_array.shape) == 3 and gt_array.shape[2] == 1:
            gt_array = gt_array[:, :, 0]
        
        gt_array = gt_array.astype(int)
        
        # Extract wavelength information for discriminative bands
        wavelengths = None
        if hasattr(hsi_img, 'metadata') and 'wavelength' in hsi_img.metadata:
            wavelengths = [float(w) for w in hsi_img.metadata['wavelength']]
            print(f"ğŸŒŠ Wavelength range: {min(wavelengths):.1f} - {max(wavelengths):.1f} nm")
        
        print(f"âœ… HSI Shape: {hsi_array.shape}")
        print(f"âœ… GT Shape: {gt_array.shape}")
        
        return hsi_array, gt_array, hsi_img, wavelengths
        
    except Exception as e:
        print(f"âŒ Lá»—i khi táº£i dá»¯ liá»‡u: {str(e)}")
        return None, None, None, None

def get_discriminative_bands(wavelengths):
    """
    XÃ¡c Ä‘á»‹nh cÃ¡c bands phÃ¢n biá»‡t dá»±a trÃªn phÃ¢n tÃ­ch tá»« bÆ°á»›c 2
    Tá»« advanced discrimination analysis: [691.497986, 695.872009, 693.684998]
    """
    if wavelengths is None:
        return None, None
    
    # Discriminative wavelengths tá»« analysis
    target_wavelengths = [691.497986, 695.872009, 693.684998]
    wavelengths_array = np.array(wavelengths)
    
    # TÃ¬m indices gáº§n nháº¥t cho cÃ¡c wavelengths quan trá»ng
    discriminative_indices = []
    discriminative_wavelengths = []
    
    for target_wl in target_wavelengths:
        closest_idx = np.argmin(np.abs(wavelengths_array - target_wl))
        discriminative_indices.append(closest_idx)
        discriminative_wavelengths.append(wavelengths_array[closest_idx])
    
    print(f"\nğŸ¯ DISCRIMINATIVE BANDS MAPPING:")
    for i, (target, actual, idx) in enumerate(zip(target_wavelengths, discriminative_wavelengths, discriminative_indices)):
        print(f"   Band {i+1}: Target {target:.1f}nm â†’ Actual {actual:.1f}nm (Index {idx})")
    
    return discriminative_indices, discriminative_wavelengths

def step3_preprocessing(hsi_array, apply_smoothing=False):
    """
    BÆ°á»›c 3: Tiá»n xá»­ lÃ½ dá»¯ liá»‡u HSI
    """
    print("\n" + "=" * 60)
    print("ğŸ§¹ BÆ¯á»šC 3: TIá»€N Xá»¬ LÃ Dá»® LIá»†U")
    print("=" * 60)
    
    print(f"ğŸ“Š Dá»¯ liá»‡u gá»‘c - Min: {np.min(hsi_array):.4f}, Max: {np.max(hsi_array):.4f}")
    
    # 3.1: Savitzky-Golay Smoothing (tÃ¹y chá»n)
    if apply_smoothing:
        print("ğŸ”„ Ãp dá»¥ng Savitzky-Golay smoothing...")
        hsi_smoothed = np.zeros_like(hsi_array)
        
        for i in tqdm(range(hsi_array.shape[0]), desc="Smoothing rows"):
            for j in range(hsi_array.shape[1]):
                spectrum = hsi_array[i, j, :]
                # Ãp dá»¥ng SG filter vá»›i window=5, polynomial=2
                try:
                    smoothed_spectrum = savgol_filter(spectrum, window_length=5, polyorder=2)
                    hsi_smoothed[i, j, :] = smoothed_spectrum
                except:
                    hsi_smoothed[i, j, :] = spectrum  # Giá»¯ nguyÃªn náº¿u lá»—i
        
        hsi_array = hsi_smoothed
        print("âœ… Smoothing hoÃ n thÃ nh!")
    else:
        print("â­ï¸ Bá» qua Savitzky-Golay smoothing (dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lá»c)")
    
    # 3.2: Chuáº©n hÃ³a dá»¯ liá»‡u vá» [0, 1]
    print("ğŸ”„ Chuáº©n hÃ³a dá»¯ liá»‡u vá» [0, 1]...")
    
    # TÃ¬m min/max toÃ n cá»¥c
    global_min = float(np.min(hsi_array))
    global_max = float(np.max(hsi_array))
    
    print(f"   Global Min: {global_min:.4f}")
    print(f"   Global Max: {global_max:.4f}")
    
    # Chuáº©n hÃ³a
    hsi_normalized = (hsi_array - global_min) / (global_max - global_min)
    hsi_normalized = np.clip(hsi_normalized, 0, 1)
    
    print(f"âœ… Sau chuáº©n hÃ³a - Min: {np.min(hsi_normalized):.4f}, Max: {np.max(hsi_normalized):.4f}")
    
    return hsi_normalized, global_min, global_max

def step3_label_processing(gt_array):
    """
    BÆ°á»›c 3: Xá»­ lÃ½ nhÃ£n - chuyá»ƒn tá»« (1,2,3) thÃ nh (0,1,2)
    """
    print("\nğŸ·ï¸ Xá»¬ LÃ NHÃƒN:")
    
    # Hiá»ƒn thá»‹ phÃ¢n bá»‘ gá»‘c
    unique_labels, counts = np.unique(gt_array, return_counts=True)
    print("ğŸ“Š PhÃ¢n bá»‘ nhÃ£n gá»‘c:")
    class_names_old = ['Unclassified', 'Low N2', 'Medium N2', 'High N2']
    for label, count in zip(unique_labels, counts):
        if label < len(class_names_old):
            print(f"   {class_names_old[label]} (Lá»›p {label}): {count:,} pixels")
    
    # Táº¡o nhÃ£n má»›i cho Deep Learning
    gt_processed = gt_array.copy()
    
    # Chuyá»ƒn Ä‘á»•i: 1â†’1, 2â†’2, 3â†’3, giá»¯ nguyÃªn 0 (Unclassified)
    # Sau Ä‘Ã³ sáº½ trá»« 1 cho cÃ¡c class training Ä‘á»ƒ cÃ³ 0,1,2
    # KhÃ´ng cáº§n Ã¡nh xáº¡ á»Ÿ Ä‘Ã¢y
    
    # Hiá»ƒn thá»‹ káº¿t quáº£
    print("\nğŸ“‹ Ãnh xáº¡ nhÃ£n cho Deep Learning:")
    print("   Lá»›p 1 (Low N2)    â†’ NhÃ£n 0 (sau khi trá»« 1)")
    print("   Lá»›p 2 (Medium N2) â†’ NhÃ£n 1 (sau khi trá»« 1)") 
    print("   Lá»›p 3 (High N2)   â†’ NhÃ£n 2 (sau khi trá»« 1)")
    print("   Lá»›p 0 (Unclassified) â†’ Loáº¡i bá» khá»i training")
    
    # Thá»‘ng kÃª nhÃ£n má»›i (chá»‰ cÃ¡c class cÃ³ thá»ƒ train Ä‘Æ°á»£c)
    training_mask = gt_processed > 0  # Loáº¡i bá» Unclassified (0)
    
    # Hiá»ƒn thá»‹ phÃ¢n bá»‘ sau khi táº¡o training mask
    training_labels = gt_processed[training_mask] - 1  # Chuyá»ƒn vá» 0,1,2
    unique_mapped, counts_mapped = np.unique(training_labels, return_counts=True)
    print(f"\nğŸ“ˆ PhÃ¢n bá»‘ nhÃ£n training (sau khi trá»« 1):")
    class_names_mapped = {0: 'Low N2', 1: 'Medium N2', 2: 'High N2'}
    for label, count in zip(unique_mapped, counts_mapped):
        if label in class_names_mapped:
            print(f"   {class_names_mapped[label]} (NhÃ£n {label}): {count:,} pixels")
    
    total_training = np.sum(counts_mapped)
    print(f"ğŸ“Š Tá»•ng sá»‘ pixels training: {total_training:,}")
    
    return gt_processed, training_mask

def step4_extract_patches(hsi_normalized, gt_processed, training_mask, patch_size=9, max_samples_per_class=50000):
    """
    BÆ°á»›c 4: TrÃ­ch xuáº¥t 3D patches tá»« dá»¯ liá»‡u HSI
    """
    print("\n" + "=" * 60)
    print("ğŸ¯ BÆ¯á»šC 4: TRÃCH XUáº¤T 3D PATCHES")
    print("=" * 60)
    
    print(f"ğŸ“ Patch size: {patch_size}x{patch_size}x{hsi_normalized.shape[2]}")
    print(f"ğŸ² Max samples per class: {max_samples_per_class:,}")
    
    height, width, n_bands = hsi_normalized.shape
    half_patch = patch_size // 2
    
    patches = []
    labels = []
    
    # Táº¡o class mapping cho balanced sampling
    class_pixels = {}
    for class_id in range(3):  # 0: Low, 1: Medium, 2: High
        # Sá»­ dá»¥ng nhÃ£n gá»‘c (1,2,3) Ä‘á»ƒ tÃ¬m pixels, sau Ä‘Ã³ Ã¡nh xáº¡ thÃ nh (0,1,2)
        original_class_id = class_id + 1  # 0â†’1, 1â†’2, 2â†’3
        mask = (gt_processed == original_class_id) & training_mask
        indices = np.where(mask)
        class_pixels[class_id] = list(zip(indices[0], indices[1]))
        print(f"ğŸ” Class {class_id} ({['Low', 'Medium', 'High'][class_id]} N2): {len(class_pixels[class_id]):,} pixels")
    
    # Balanced sampling tá»« má»—i class
    for class_id in range(3):
        pixels = class_pixels[class_id]
        
        # Random sample náº¿u cÃ³ quÃ¡ nhiá»u pixels
        if len(pixels) > max_samples_per_class:
            pixels = np.random.choice(len(pixels), max_samples_per_class, replace=False)
            pixels = [class_pixels[class_id][i] for i in pixels]
            print(f"   ğŸ“Š Downsampled to {len(pixels):,} pixels")
        
        class_patches = []
        valid_patches = 0
        
        for i, j in tqdm(pixels, desc=f"Extracting Class {class_id} patches"):
            # Kiá»ƒm tra boundaries
            if (i >= half_patch and i < height - half_patch and 
                j >= half_patch and j < width - half_patch):
                
                # TrÃ­ch xuáº¥t patch 3D
                patch = hsi_normalized[i-half_patch:i+half_patch+1, 
                                     j-half_patch:j+half_patch+1, :]
                
                # Kiá»ƒm tra patch shape
                if patch.shape == (patch_size, patch_size, n_bands):
                    class_patches.append(patch)
                    valid_patches += 1
        
        # ThÃªm vÃ o dataset
        patches.extend(class_patches)
        labels.extend([class_id] * len(class_patches))
        
        print(f"âœ… Class {class_id}: {len(class_patches):,} patches extracted")
    
    # Convert to numpy arrays
    patches = np.array(patches, dtype=np.float32)
    labels = np.array(labels, dtype=np.int32)
    
    print(f"\nğŸ“Š DATASET SUMMARY:")
    print(f"   ğŸ“ Patches shape: {patches.shape}")
    print(f"   ğŸ·ï¸ Labels shape: {labels.shape}")
    print(f"   ğŸ’¾ Memory usage: {patches.nbytes / (1024**2):.1f} MB")
    
    # Thá»‘ng kÃª phÃ¢n bá»‘ final
    unique_final, counts_final = np.unique(labels, return_counts=True)
    for label, count in zip(unique_final, counts_final):
        print(f"   Class {label}: {count:,} patches ({count/len(labels)*100:.1f}%)")
    
    return patches, labels

def step4_train_val_test_split(patches, labels, test_size=0.15, val_size=0.15):
    """
    BÆ°á»›c 4: Chia dataset thÃ nh train/validation/test
    """
    print("\n" + "=" * 60)
    print("ğŸ”„ CHIA DATASET TRAIN/VAL/TEST")
    print("=" * 60)
    
    # TÃ­nh toÃ¡n tá»· lá»‡
    train_size = 1.0 - test_size - val_size
    print(f"ğŸ“Š Tá»· lá»‡ chia: Train {train_size:.0%} / Val {val_size:.0%} / Test {test_size:.0%}")
    
    # Chia train vs (val+test) trÆ°á»›c
    X_train, X_temp, y_train, y_temp = train_test_split(
        patches, labels, 
        test_size=(test_size + val_size),
        stratify=labels,
        random_state=42
    )
    
    # Chia val vs test
    relative_val_size = val_size / (test_size + val_size)
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp,
        test_size=(1 - relative_val_size),
        stratify=y_temp,
        random_state=42
    )
    
    # Thá»‘ng kÃª káº¿t quáº£
    print(f"\nğŸ“ˆ Káº¾T QUáº¢ CHIA DATASET:")
    print(f"   ğŸ‹ï¸ Training:   {X_train.shape[0]:,} samples ({X_train.shape[0]/len(patches)*100:.1f}%)")
    print(f"   âœ… Validation: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(patches)*100:.1f}%)")
    print(f"   ğŸ§ª Testing:    {X_test.shape[0]:,} samples ({X_test.shape[0]/len(patches)*100:.1f}%)")
    
    # Kiá»ƒm tra stratification
    print(f"\nğŸ¯ KIá»‚M TRA STRATIFICATION:")
    datasets = [('Train', y_train), ('Val', y_val), ('Test', y_test)]
    
    for name, y_data in datasets:
        unique_labels, counts = np.unique(y_data, return_counts=True)
        percentages = counts / len(y_data) * 100
        print(f"   {name:10}: ", end="")
        for label, pct in zip(unique_labels, percentages):
            print(f"Class {label}: {pct:.1f}%  ", end="")
        print()
    
    return X_train, X_val, X_test, y_train, y_val, y_test

def save_dataset(X_train, X_val, X_test, y_train, y_val, y_test, 
                 global_min, global_max, discriminative_indices=None, discriminative_wavelengths=None, 
                 save_dir="processed_data"):
    """
    LÆ°u dataset Ä‘Ã£ xá»­ lÃ½
    """
    print("\n" + "=" * 60)
    print("ğŸ’¾ LÆ¯U DATASET")
    print("=" * 60)
    
    # Táº¡o thÆ° má»¥c náº¿u chÆ°a cÃ³, xÃ³a dá»¯ liá»‡u cÅ© náº¿u cÃ³
    if os.path.exists(save_dir):
        print(f"ğŸ—‘ï¸ XÃ³a dá»¯ liá»‡u cÅ© trong thÆ° má»¥c: {save_dir}")
        import shutil
        shutil.rmtree(save_dir)
    
    os.makedirs(save_dir)
    print(f"ğŸ“ Táº¡o thÆ° má»¥c má»›i: {save_dir}")
    
    # LÆ°u cÃ¡c file
    datasets = {
        'X_train.npy': X_train,
        'X_val.npy': X_val, 
        'X_test.npy': X_test,
        'y_train.npy': y_train,
        'y_val.npy': y_val,
        'y_test.npy': y_test
    }
    
    total_size = 0
    for filename, data in datasets.items():
        filepath = os.path.join(save_dir, filename)
        np.save(filepath, data)
        size_mb = os.path.getsize(filepath) / (1024**2)
        total_size += size_mb
        print(f"âœ… Saved {filename:12} - {data.shape} - {size_mb:.1f} MB")
    
    # LÆ°u metadata vá»›i discriminative bands info
    metadata = {
        'patch_size': X_train.shape[1],  # Assuming square patches
        'n_bands': X_train.shape[3],
        'n_classes': len(np.unique(y_train)),
        'global_min': global_min,
        'global_max': global_max,
        'class_names': ['Low N2', 'Medium N2', 'High N2'],
        'total_samples': len(X_train) + len(X_val) + len(X_test),
        'discriminative_bands': {
            'indices': discriminative_indices,
            'wavelengths': discriminative_wavelengths,
            'target_wavelengths': [691.497986, 695.872009, 693.684998]
        } if discriminative_indices is not None else None
    }
    
    metadata_path = os.path.join(save_dir, 'metadata.pkl')
    with open(metadata_path, 'wb') as f:
        pickle.dump(metadata, f)
    
    print(f"âœ… Saved metadata.pkl")
    print(f"ğŸ’¾ Total dataset size: {total_size:.1f} MB")
    print(f"ğŸ“ Saved in: {os.path.abspath(save_dir)}")

def main():
    """
    HÃ m chÃ­nh thá»±c hiá»‡n toÃ n bá»™ quy trÃ¬nh BÆ°á»›c 3 & 4
    """
    start_time = time.time()
    
    # Load data
    hsi_array, gt_array, hsi_img, wavelengths = load_data()
    if hsi_array is None:
        return
    
    # Extract discriminative bands information
    discriminative_indices, discriminative_wavelengths = get_discriminative_bands(wavelengths)
    
    # BÆ°á»›c 3.1: Preprocessing HSI
    hsi_normalized, global_min, global_max = step3_preprocessing(
        hsi_array, apply_smoothing=False
    )
    
    # BÆ°á»›c 3.2: Label processing  
    gt_processed, training_mask = step3_label_processing(gt_array)
    
    # BÆ°á»›c 4.1: Extract 3D patches
    patches, labels = step4_extract_patches(
        hsi_normalized, gt_processed, training_mask, 
        patch_size=9, max_samples_per_class=50000
    )
    
    # BÆ°á»›c 4.2: Train/Val/Test split
    X_train, X_val, X_test, y_train, y_val, y_test = step4_train_val_test_split(
        patches, labels, test_size=0.15, val_size=0.15
    )
    
    # BÆ°á»›c 4.3: Save dataset
    save_dataset(X_train, X_val, X_test, y_train, y_val, y_test,
                 global_min, global_max, discriminative_indices, discriminative_wavelengths)
    
    # Summary
    elapsed_time = time.time() - start_time
    print("\n" + "=" * 60)
    print("ğŸ‰ HOÃ€N THÃ€NH BÆ¯á»šC 3 & 4")
    print("=" * 60)
    print("âœ… Tiá»n xá»­ lÃ½ dá»¯ liá»‡u hoÃ n thÃ nh")
    print("âœ… 3D patches Ä‘Ã£ Ä‘Æ°á»£c táº¡o")
    print("âœ… Dataset Ä‘Ã£ Ä‘Æ°á»£c chia train/val/test")
    print("âœ… Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÆ°u")
    print(f"â±ï¸ Thá»i gian thá»±c hiá»‡n: {elapsed_time/60:.1f} phÃºt")
    print("â¡ï¸ Sáºµn sÃ ng cho BÆ°á»›c 5: XÃ¢y dá»±ng 3D-ResNet")
    print("=" * 60)

if __name__ == "__main__":
    main()
