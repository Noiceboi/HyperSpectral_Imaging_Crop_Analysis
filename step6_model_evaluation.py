"""
BÆ°á»›c 6: ÄÃ¡nh giÃ¡ vÃ  Test MÃ´ hÃ¬nh 3D-ResNet
Má»¥c tiÃªu: ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t mÃ´ hÃ¬nh trÃªn test set vÃ  táº¡o bÃ¡o cÃ¡o chi tiáº¿t
"""

import torch
import torch.nn as nn
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
import pickle
from sklearn.metrics import (classification_report, confusion_matrix, 
                           accuracy_score, precision_recall_fscore_support)
from sklearn.preprocessing import LabelEncoder
import pandas as pd
from torch.utils.data import DataLoader, TensorDataset

# Import mÃ´ hÃ¬nh tá»« bÆ°á»›c 5
import sys
sys.path.append('.')
from step5_3d_resnet_pytorch import ResNet3D_Eggplant

print("=" * 60)
print("ğŸš€ BÆ¯á»šC 6: ÄÃNH GIÃ VÃ€ TEST MÃ” HÃŒNH")
print("=" * 60)

# Kiá»ƒm tra GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ¯ Device: {device}")

def load_test_data():
    """
    6.1. Táº£i dá»¯ liá»‡u test
    """
    print("\n" + "=" * 60)
    print("ğŸ“‚ BÆ¯á»šC 6.1: Táº¢I Dá»® LIá»†U TEST")
    print("=" * 60)
    
    DATA_DIR = 'processed_data'
    
    try:
        print("ğŸ”„ Äang táº£i test dataset...")
        
        # Táº£i dá»¯ liá»‡u test
        X_test = np.load(os.path.join(DATA_DIR, 'X_test.npy'))
        y_test = np.load(os.path.join(DATA_DIR, 'y_test.npy'))
        
        # Táº£i metadata
        with open(os.path.join(DATA_DIR, 'metadata.pkl'), 'rb') as f:
            metadata = pickle.load(f)
        
        print("âœ… Táº£i dá»¯ liá»‡u test thÃ nh cÃ´ng!")
        print(f"ğŸ“Š Test set: {X_test.shape} | Labels: {y_test.shape}")
        
        # Kiá»ƒm tra phÃ¢n bá»‘ nhÃ£n test
        unique_test, counts_test = np.unique(y_test, return_counts=True)
        print("ğŸ“Š PhÃ¢n bá»‘ nhÃ£n Test:")
        class_names = metadata['class_names']
        for label, count in zip(unique_test, counts_test):
            if label < len(class_names):
                print(f"   {class_names[label]} (Class {label}): {count:,} samples")
        
        return X_test, y_test, metadata
        
    except Exception as e:
        print(f"âŒ Lá»—i khi táº£i dá»¯ liá»‡u test: {str(e)}")
        return None, None, None

def prepare_test_data_pytorch(X_test, y_test):
    """
    6.2. Chuáº©n bá»‹ dá»¯ liá»‡u test cho PyTorch
    """
    print("\nğŸ”„ CHUáº¨N Bá»Š Dá»® LIá»†U TEST CHO PYTORCH:")
    
    # Chuyá»ƒn Ä‘á»•i sang tensor vÃ  thÃªm channel dimension
    X_test_tensor = torch.FloatTensor(X_test).unsqueeze(1)  # (N, 1, 9, 9, 277)
    
    # Permute Ä‘á»ƒ cÃ³ format chuáº©n cá»§a PyTorch 3D: (N, C, D, H, W)
    X_test_tensor = X_test_tensor.permute(0, 1, 4, 2, 3)  # (N, 1, 277, 9, 9)
    
    y_test_tensor = torch.LongTensor(y_test)
    
    print(f"âœ… Test tensor conversion hoÃ n thÃ nh!")
    print(f"ğŸ“Š X_test tensor shape: {X_test_tensor.shape}")
    print(f"ğŸ“Š y_test tensor shape: {y_test_tensor.shape}")
    
    return X_test_tensor, y_test_tensor

def load_trained_model(model_path, num_classes):
    """
    6.3. Táº£i mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n
    """
    print("\n" + "=" * 60)
    print("ğŸ”„ BÆ¯á»šC 6.3: Táº¢I MÃ” HÃŒNH ÄÃƒ HUáº¤N LUYá»†N")
    print("=" * 60)
    
    try:
        # Táº¡o mÃ´ hÃ¬nh vá»›i cÃ¹ng architecture
        model = ResNet3D_Eggplant(num_classes=num_classes, input_channels=1)
        
        # Táº£i trá»ng sá»‘ Ä‘Ã£ huáº¥n luyá»‡n
        model.load_state_dict(torch.load(model_path, map_location=device))
        model = model.to(device)
        model.eval()  # Cháº¿ Ä‘á»™ evaluation
        
        print(f"âœ… Táº£i mÃ´ hÃ¬nh thÃ nh cÃ´ng tá»«: {model_path}")
        print(f"ğŸ¯ MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c chuyá»ƒn sang device: {device}")
        
        return model
        
    except Exception as e:
        print(f"âŒ Lá»—i khi táº£i mÃ´ hÃ¬nh: {str(e)}")
        return None

def evaluate_model(model, test_loader, class_names):
    """
    6.4. ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn test set
    """
    print("\n" + "=" * 60)
    print("ğŸ§ª BÆ¯á»šC 6.4: ÄÃNH GIÃ MÃ” HÃŒNH TRÃŠN TEST SET")
    print("=" * 60)
    
    model.eval()
    all_predictions = []
    all_targets = []
    all_probabilities = []
    
    print("ğŸ”„ Äang thá»±c hiá»‡n prediction...")
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            
            # Forward pass
            output = model(data)
            probabilities = torch.nn.functional.softmax(output, dim=1)
            
            # Láº¥y predictions
            _, predicted = torch.max(output, 1)
            
            # LÆ°u trá»¯ káº¿t quáº£
            all_predictions.extend(predicted.cpu().numpy())
            all_targets.extend(target.cpu().numpy())
            all_probabilities.extend(probabilities.cpu().numpy())
    
    # Chuyá»ƒn sang numpy arrays
    all_predictions = np.array(all_predictions)
    all_targets = np.array(all_targets)
    all_probabilities = np.array(all_probabilities)
    
    print("âœ… HoÃ n thÃ nh prediction!")
    
    # TÃ­nh toÃ¡n metrics
    print("\nğŸ“Š TÃNH TOÃN METRICS:")
    
    # Overall accuracy
    accuracy = accuracy_score(all_targets, all_predictions)
    print(f"ğŸ¯ Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    # Per-class metrics
    precision, recall, f1, support = precision_recall_fscore_support(
        all_targets, all_predictions, average=None, labels=range(len(class_names))
    )
    
    # Macro vÃ  weighted averages
    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(
        all_targets, all_predictions, average='macro'
    )
    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(
        all_targets, all_predictions, average='weighted'
    )
    
    print(f"\nğŸ“ˆ PER-CLASS METRICS:")
    print(f"{'Class':<15} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}")
    print("-" * 65)
    for i, class_name in enumerate(class_names):
        print(f"{class_name:<15} {precision[i]:<10.4f} {recall[i]:<10.4f} {f1[i]:<10.4f} {support[i]:<10}")
    
    print(f"\nğŸ“Š MACRO AVERAGES:")
    print(f"   Precision: {precision_macro:.4f}")
    print(f"   Recall: {recall_macro:.4f}")
    print(f"   F1-Score: {f1_macro:.4f}")
    
    print(f"\nâš–ï¸ WEIGHTED AVERAGES:")
    print(f"   Precision: {precision_weighted:.4f}")
    print(f"   Recall: {recall_weighted:.4f}")
    print(f"   F1-Score: {f1_weighted:.4f}")
    
    return {
        'predictions': all_predictions,
        'targets': all_targets,
        'probabilities': all_probabilities,
        'accuracy': accuracy,
        'precision_per_class': precision,
        'recall_per_class': recall,
        'f1_per_class': f1,
        'support_per_class': support,
        'precision_macro': precision_macro,
        'recall_macro': recall_macro,
        'f1_macro': f1_macro,
        'precision_weighted': precision_weighted,
        'recall_weighted': recall_weighted,
        'f1_weighted': f1_weighted
    }

def create_confusion_matrix(results, class_names, save_path='models/confusion_matrix.png'):
    """
    6.5. Táº¡o Confusion Matrix
    """
    print("\n" + "=" * 60)
    print("ğŸ“Š BÆ¯á»šC 6.5: Táº O CONFUSION MATRIX")
    print("=" * 60)
    
    # TÃ­nh confusion matrix
    cm = confusion_matrix(results['targets'], results['predictions'])
    
    # TÃ­nh pháº§n trÄƒm
    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
    
    # Táº¡o figure vá»›i 2 subplots
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
    
    # Confusion matrix vá»›i sá»‘ lÆ°á»£ng máº«u
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names, ax=ax1)
    ax1.set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Predicted Label')
    ax1.set_ylabel('True Label')
    
    # Confusion matrix vá»›i pháº§n trÄƒm
    sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names, ax=ax2)
    ax2.set_title('Confusion Matrix (Percentages)', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Predicted Label')
    ax2.set_ylabel('True Label')
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š Confusion matrix Ä‘Ã£ Ä‘Æ°á»£c lÆ°u: {save_path}")
    
    plt.show()
    
    # In thÃ´ng tin chi tiáº¿t vá» confusion matrix
    print(f"\nğŸ“ˆ CONFUSION MATRIX ANALYSIS:")
    print(f"{'Class':<15} {'True Pos':<10} {'False Pos':<10} {'False Neg':<10} {'True Neg':<10}")
    print("-" * 65)
    
    for i, class_name in enumerate(class_names):
        tp = cm[i, i]
        fp = cm[:, i].sum() - tp
        fn = cm[i, :].sum() - tp
        tn = cm.sum() - tp - fp - fn
        
        print(f"{class_name:<15} {tp:<10} {fp:<10} {fn:<10} {tn:<10}")

def create_classification_report(results, class_names, save_path='models/classification_report.txt'):
    """
    6.6. Táº¡o bÃ¡o cÃ¡o phÃ¢n loáº¡i chi tiáº¿t
    """
    print("\n" + "=" * 60)
    print("ğŸ“‹ BÆ¯á»šC 6.6: Táº O BÃO CÃO PHÃ‚N LOáº I")
    print("=" * 60)
    
    # Táº¡o classification report
    report = classification_report(
        results['targets'], 
        results['predictions'], 
        target_names=class_names,
        digits=4
    )
    
    print("ğŸ“Š CLASSIFICATION REPORT:")
    print(report)
    
    # LÆ°u bÃ¡o cÃ¡o vÃ o file
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write("=" * 60 + "\n")
        f.write("BÃO CÃO ÄÃNH GIÃ MÃ” HÃŒNH 3D-RESNET\n")
        f.write("PHÃ‚N LOáº I TÃŒNH TRáº NG DINH DÆ¯á» NG N2 CÃ‚Y CÃ€ TÃM\n")
        f.write("=" * 60 + "\n\n")
        
        f.write(f"Overall Accuracy: {results['accuracy']:.4f} ({results['accuracy']*100:.2f}%)\n\n")
        
        f.write("CLASSIFICATION REPORT:\n")
        f.write("-" * 40 + "\n")
        f.write(report)
        f.write("\n\n")
        
        f.write("PER-CLASS DETAILED METRICS:\n")
        f.write("-" * 40 + "\n")
        for i, class_name in enumerate(class_names):
            f.write(f"\n{class_name}:\n")
            f.write(f"  Precision: {results['precision_per_class'][i]:.4f}\n")
            f.write(f"  Recall: {results['recall_per_class'][i]:.4f}\n")
            f.write(f"  F1-Score: {results['f1_per_class'][i]:.4f}\n")
            f.write(f"  Support: {results['support_per_class'][i]}\n")
    
    print(f"ğŸ“„ BÃ¡o cÃ¡o Ä‘Ã£ Ä‘Æ°á»£c lÆ°u: {save_path}")

def create_prediction_confidence_analysis(results, class_names):
    """
    6.7. PhÃ¢n tÃ­ch Ä‘á»™ tin cáº­y cá»§a predictions
    """
    print("\n" + "=" * 60)
    print("ğŸ” BÆ¯á»šC 6.7: PHÃ‚N TÃCH Äá»˜ TIN Cáº¬Y PREDICTIONS")
    print("=" * 60)
    
    # TÃ­nh confidence scores (max probability)
    confidence_scores = np.max(results['probabilities'], axis=1)
    
    # Táº¡o figure vá»›i multiple subplots
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. Histogram cá»§a confidence scores
    axes[0, 0].hist(confidence_scores, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
    axes[0, 0].set_title('Distribution of Prediction Confidence', fontweight='bold')
    axes[0, 0].set_xlabel('Confidence Score')
    axes[0, 0].set_ylabel('Frequency')
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. Confidence by class
    confidence_by_class = []
    for i, class_name in enumerate(class_names):
        class_mask = results['targets'] == i
        class_confidences = confidence_scores[class_mask]
        confidence_by_class.append(class_confidences)
    
    axes[0, 1].boxplot(confidence_by_class, labels=class_names)
    axes[0, 1].set_title('Confidence Distribution by Class', fontweight='bold')
    axes[0, 1].set_ylabel('Confidence Score')
    axes[0, 1].tick_params(axis='x', rotation=45)
    
    # 3. Accuracy vs Confidence
    confidence_bins = np.linspace(0, 1, 11)
    accuracies_by_confidence = []
    counts_by_confidence = []
    
    for i in range(len(confidence_bins)-1):
        low, high = confidence_bins[i], confidence_bins[i+1]
        mask = (confidence_scores >= low) & (confidence_scores < high)
        if mask.sum() > 0:
            acc = (results['predictions'][mask] == results['targets'][mask]).mean()
            accuracies_by_confidence.append(acc)
            counts_by_confidence.append(mask.sum())
        else:
            accuracies_by_confidence.append(0)
            counts_by_confidence.append(0)
    
    bin_centers = (confidence_bins[:-1] + confidence_bins[1:]) / 2
    axes[1, 0].bar(bin_centers, accuracies_by_confidence, width=0.08, alpha=0.7, color='lightgreen')
    axes[1, 0].set_title('Accuracy vs Confidence Level', fontweight='bold')
    axes[1, 0].set_xlabel('Confidence Level')
    axes[1, 0].set_ylabel('Accuracy')
    axes[1, 0].set_ylim(0, 1)
    
    # 4. Error analysis - low confidence predictions
    low_confidence_threshold = 0.6
    low_conf_mask = confidence_scores < low_confidence_threshold
    error_mask = results['predictions'] != results['targets']
    
    low_conf_errors = low_conf_mask & error_mask
    high_conf_errors = (~low_conf_mask) & error_mask
    
    error_types = ['Low Confidence\nErrors', 'High Confidence\nErrors', 'Correct\nPredictions']
    error_counts = [low_conf_errors.sum(), high_conf_errors.sum(), 
                   (results['predictions'] == results['targets']).sum()]
    
    colors = ['red', 'orange', 'green']
    axes[1, 1].pie(error_counts, labels=error_types, colors=colors, autopct='%1.1f%%')
    axes[1, 1].set_title('Error Analysis by Confidence', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('models/confidence_analysis.png', dpi=300, bbox_inches='tight')
    print("ğŸ“Š Biá»ƒu Ä‘á»“ confidence analysis Ä‘Ã£ Ä‘Æ°á»£c lÆ°u: models/confidence_analysis.png")
    
    plt.show()
    
    # In thá»‘ng kÃª
    print(f"\nğŸ“ˆ CONFIDENCE STATISTICS:")
    print(f"   Mean Confidence: {confidence_scores.mean():.4f}")
    print(f"   Std Confidence: {confidence_scores.std():.4f}")
    print(f"   Min Confidence: {confidence_scores.min():.4f}")
    print(f"   Max Confidence: {confidence_scores.max():.4f}")
    
    print(f"\nâš ï¸ LOW CONFIDENCE ANALYSIS (< {low_confidence_threshold}):")
    print(f"   Low confidence predictions: {low_conf_mask.sum()}")
    print(f"   Low confidence errors: {low_conf_errors.sum()}")
    print(f"   High confidence errors: {high_conf_errors.sum()}")

def main():
    """
    HÃ m chÃ­nh thá»±c hiá»‡n toÃ n bá»™ quy trÃ¬nh BÆ°á»›c 6
    """
    print("ğŸ¯ Báº¯t Ä‘áº§u Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh...")
    
    # BÆ°á»›c 6.1: Táº£i dá»¯ liá»‡u test
    X_test, y_test, metadata = load_test_data()
    if X_test is None:
        return
    
    # BÆ°á»›c 6.2: Chuáº©n bá»‹ dá»¯ liá»‡u test
    X_test_tensor, y_test_tensor = prepare_test_data_pytorch(X_test, y_test)
    
    # Táº¡o test DataLoader
    BATCH_SIZE = 64
    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)
    
    print(f"ğŸ“¦ Test DataLoader created: {len(test_loader)} batches")
    
    # BÆ°á»›c 6.3: Táº£i mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n
    MODEL_PATH = 'models/best_3d_resnet_pytorch.pth'
    NUM_CLASSES = metadata['n_classes']
    CLASS_NAMES = metadata['class_names']
    
    model = load_trained_model(MODEL_PATH, NUM_CLASSES)
    if model is None:
        return
    
    # BÆ°á»›c 6.4: ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh
    results = evaluate_model(model, test_loader, CLASS_NAMES)
    
    # BÆ°á»›c 6.5: Táº¡o confusion matrix
    create_confusion_matrix(results, CLASS_NAMES)
    
    # BÆ°á»›c 6.6: Táº¡o bÃ¡o cÃ¡o phÃ¢n loáº¡i
    create_classification_report(results, CLASS_NAMES)
    
    # BÆ°á»›c 6.7: PhÃ¢n tÃ­ch Ä‘á»™ tin cáº­y
    create_prediction_confidence_analysis(results, CLASS_NAMES)
    
    # Tá»•ng káº¿t
    print("\n" + "=" * 60)
    print("ğŸ‰ HOÃ€N THÃ€NH BÆ¯á»šC 6")
    print("=" * 60)
    print("âœ… ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh trÃªn test set hoÃ n táº¥t")
    print("âœ… Confusion matrix Ä‘Ã£ Ä‘Æ°á»£c táº¡o")
    print("âœ… BÃ¡o cÃ¡o phÃ¢n loáº¡i Ä‘Ã£ Ä‘Æ°á»£c lÆ°u")
    print("âœ… PhÃ¢n tÃ­ch Ä‘á»™ tin cáº­y hoÃ n thÃ nh")
    print(f"ğŸ¯ Test Accuracy: {results['accuracy']*100:.2f}%")
    print(f"ğŸ“Š F1-Score (Macro): {results['f1_macro']:.4f}")
    print(f"ğŸ“Š F1-Score (Weighted): {results['f1_weighted']:.4f}")
    print("=" * 60)

if __name__ == "__main__":
    main()
