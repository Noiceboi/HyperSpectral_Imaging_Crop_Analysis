"""
BÆ°á»›c 5: XÃ¢y dá»±ng vÃ  Huáº¥n luyá»‡n MÃ´ hÃ¬nh 3D-ResNet
Má»¥c tiÃªu: XÃ¢y dá»±ng kiáº¿n trÃºc 3D-ResNet Ä‘á»ƒ phÃ¢n loáº¡i tÃ¬nh tráº¡ng dinh dÆ°á»¡ng N2 cá»§a cÃ¢y cÃ  tÃ­m
"""

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, Conv3D, BatchNormalization, ReLU, Add, 
                                     GlobalAveragePooling3D, Dense, MaxPooling3D)
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical
import numpy as np
import os
import matplotlib.pyplot as plt
import pickle
import time

# Cáº¥u hÃ¬nh GPU
print("=" * 60)
print("ğŸš€ BÆ¯á»šC 5: XÃ‚Y Dá»°NG VÃ€ HUáº¤N LUYá»†N 3D-RESNET")
print("=" * 60)

# Kiá»ƒm tra GPU
print("ğŸ” KIá»‚M TRA GPU:")
print(f"ğŸ”§ TensorFlow version: {tf.__version__}")
print(f"ğŸ—ï¸ Built with CUDA: {tf.test.is_built_with_cuda()}")

# Kiá»ƒm tra GPU báº±ng cáº£ 2 phÆ°Æ¡ng phÃ¡p
gpus = tf.config.experimental.list_physical_devices('GPU')
print(f"ğŸ“Š Physical GPUs found: {len(gpus)}")
for i, gpu in enumerate(gpus):
    print(f"   GPU {i}: {gpu}")

# Kiá»ƒm tra GPU availability (deprecated nhÆ°ng há»¯u Ã­ch Ä‘á»ƒ debug)
try:
    gpu_available = tf.test.is_gpu_available()
    print(f"ğŸ¯ GPU available (legacy method): {gpu_available}")
except:
    print("ğŸ¯ GPU availability check failed")

if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"âœ… TÃ¬m tháº¥y {len(gpus)} GPU(s) vÃ  Ä‘Ã£ cáº¥u hÃ¬nh memory growth")
        print("ğŸš€ Sáºµn sÃ ng sá»­ dá»¥ng GPU cho training!")
    except RuntimeError as e:
        print(f"âŒ Lá»—i cáº¥u hÃ¬nh GPU: {e}")
        print("âš ï¸ Sáº½ tiáº¿p tá»¥c vá»›i CPU")
else:
    print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y GPU, sá»­ dá»¥ng CPU")
    print("ğŸ’¡ Kiá»ƒm tra: NVIDIA driver, CUDA, cuDNN installation")

def load_processed_data():
    """
    5.1. Táº£i dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ tá»« BÆ°á»›c 4
    """
    print("\n" + "=" * 60)
    print("ğŸ“‚ BÆ¯á»šC 5.1: Táº¢I Dá»® LIá»†U ÄÃƒ Xá»¬ LÃ")
    print("=" * 60)
    
    DATA_DIR = 'processed_data'
    
    # Kiá»ƒm tra sá»± tá»“n táº¡i cá»§a thÆ° má»¥c
    if not os.path.exists(DATA_DIR):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c {DATA_DIR}")
        print("Vui lÃ²ng cháº¡y BÆ°á»›c 3 & 4 trÆ°á»›c!")
        return None, None, None, None, None
    
    try:
        print("ğŸ”„ Äang táº£i dataset...")
        
        # Táº£i dá»¯ liá»‡u training vÃ  validation
        X_train = np.load(os.path.join(DATA_DIR, 'X_train.npy'))
        y_train = np.load(os.path.join(DATA_DIR, 'y_train.npy'))
        X_val = np.load(os.path.join(DATA_DIR, 'X_val.npy'))
        y_val = np.load(os.path.join(DATA_DIR, 'y_val.npy'))
        
        # Táº£i metadata
        with open(os.path.join(DATA_DIR, 'metadata.pkl'), 'rb') as f:
            metadata = pickle.load(f)
        
        print("âœ… Táº£i dá»¯ liá»‡u thÃ nh cÃ´ng!")
        print(f"ğŸ“Š Training set: {X_train.shape} | Labels: {y_train.shape}")
        print(f"ğŸ“Š Validation set: {X_val.shape} | Labels: {y_val.shape}")
        print(f"ğŸ“Š Patch size: {metadata['patch_size']}x{metadata['patch_size']}x{metadata['n_bands']}")
        print(f"ğŸ“Š Number of classes: {metadata['n_classes']}")
        print(f"ğŸ“Š Class names: {metadata['class_names']}")
        
        return X_train, y_train, X_val, y_val, metadata
        
    except Exception as e:
        print(f"âŒ Lá»—i khi táº£i dá»¯ liá»‡u: {str(e)}")
        return None, None, None, None, None

def prepare_labels_for_training(y_train, y_val, num_classes):
    """
    5.1. Chuáº©n bá»‹ NhÃ£n cho Training (One-Hot Encoding)
    """
    print("\nğŸ·ï¸ CHUáº¨N Bá»Š NHÃƒN CHO TRAINING:")
    
    # Kiá»ƒm tra phÃ¢n bá»‘ nhÃ£n trÆ°á»›c khi chuyá»ƒn Ä‘á»•i
    unique_train, counts_train = np.unique(y_train, return_counts=True)
    unique_val, counts_val = np.unique(y_val, return_counts=True)
    
    print("ğŸ“Š PhÃ¢n bá»‘ nhÃ£n Training:")
    class_names = ['Low N2', 'Medium N2', 'High N2']
    for label, count in zip(unique_train, counts_train):
        if label < len(class_names):
            print(f"   {class_names[label]} (Class {label}): {count:,} samples")
    
    print("ğŸ“Š PhÃ¢n bá»‘ nhÃ£n Validation:")
    for label, count in zip(unique_val, counts_val):
        if label < len(class_names):
            print(f"   {class_names[label]} (Class {label}): {count:,} samples")
    
    # Chuyá»ƒn Ä‘á»•i sang One-Hot Encoding
    print(f"\nğŸ”„ Chuyá»ƒn Ä‘á»•i nhÃ£n sang One-Hot Encoding...")
    y_train_cat = to_categorical(y_train, num_classes=num_classes)
    y_val_cat = to_categorical(y_val, num_classes=num_classes)
    
    print(f"âœ… One-Hot Encoding hoÃ n thÃ nh!")
    print(f"ğŸ“Š y_train_cat shape: {y_train_cat.shape}")
    print(f"ğŸ“Š y_val_cat shape: {y_val_cat.shape}")
    print(f"ğŸ“Š VÃ­ dá»¥: nhÃ£n {y_train[0]} â†’ {y_train_cat[0]}")
    
    return y_train_cat, y_val_cat

def residual_block(input_tensor, filters, kernel_size=(3, 3, 3)):
    """
    5.2. Äá»‹nh nghÄ©a Khá»‘i dÆ° 3D (3D Residual Block) 
    Kiáº¿n trÃºc: Conv -> BN -> ReLU -> Conv -> BN -> Add -> ReLU
    """
    x = input_tensor
    
    # NhÃ¡nh chÃ­nh (main path)
    # Lá»›p tÃ­ch cháº­p thá»© nháº¥t
    main_path = Conv3D(filters, kernel_size=kernel_size, padding='same')(x)
    main_path = BatchNormalization()(main_path)
    main_path = ReLU()(main_path)
    
    # Lá»›p tÃ­ch cháº­p thá»© hai
    main_path = Conv3D(filters, kernel_size=kernel_size, padding='same')(main_path)
    main_path = BatchNormalization()(main_path)
    
    # NhÃ¡nh táº¯t (shortcut connection)
    # Náº¿u sá»‘ lÆ°á»£ng bá»™ lá»c Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra khÃ¡c nhau, dÃ¹ng 1x1x1 conv Ä‘á»ƒ khá»›p kÃ­ch thÆ°á»›c
    if x.shape[-1] != filters:
        shortcut = Conv3D(filters, kernel_size=(1, 1, 1), padding='same')(x)
        shortcut = BatchNormalization()(shortcut)
    else:
        shortcut = x
        
    # Cá»™ng nhÃ¡nh chÃ­nh vÃ  nhÃ¡nh táº¯t
    added = Add()([main_path, shortcut])
    output = ReLU()(added)
    
    return output

def build_detailed_3d_resnet(input_shape, num_classes):
    """
    5.3. XÃ¢y dá»±ng Kiáº¿n trÃºc 3D-ResNet HoÃ n chá»‰nh
    """
    print("\n" + "=" * 60)
    print("ğŸ—ï¸ BÆ¯á»šC 5.3: XÃ‚Y Dá»°NG KIáº¾N TRÃšC 3D-RESNET")
    print("=" * 60)
    
    print(f"ğŸ“ Input shape: {input_shape}")
    print(f"ğŸ¯ Number of classes: {num_classes}")
    
    # Lá»›p Ä‘áº§u vÃ o, thÃªm 1 chiá»u kÃªnh cho Conv3D
    inputs = Input(shape=input_shape + (1,))
    print(f"ğŸ“Š Input layer shape: {inputs.shape}")
    
    # --- Giai Ä‘oáº¡n 1: TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng ban Ä‘áº§u ---
    print("ğŸ”§ Giai Ä‘oáº¡n 1: TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng ban Ä‘áº§u...")
    # Sá»­ dá»¥ng kernel lá»›n hÆ¡n á»Ÿ chiá»u quang phá»• Ä‘á»ƒ náº¯m báº¯t cÃ¡c máº«u rá»™ng
    x = Conv3D(filters=32, kernel_size=(3, 3, 7), padding='same', name='initial_conv')(inputs)
    x = BatchNormalization(name='initial_bn')(x)
    x = ReLU(name='initial_relu')(x)
    x = MaxPooling3D(pool_size=(1, 1, 2), name='initial_pool')(x)  # Giáº£m chiá»u quang phá»•
    print(f"   Sau giai Ä‘oáº¡n 1: {x.shape}")
    
    # --- Giai Ä‘oáº¡n 2: CÃ¡c Khá»‘i dÆ° Ä‘áº§u tiÃªn ---
    print("ğŸ”§ Giai Ä‘oáº¡n 2: Khá»‘i residual Ä‘áº§u tiÃªn...")
    # Chá»“ng 2 khá»‘i dÆ° vá»›i 32 bá»™ lá»c
    x = residual_block(x, filters=32)
    x = residual_block(x, filters=32)
    print(f"   Sau giai Ä‘oáº¡n 2: {x.shape}")
    
    # --- Giai Ä‘oáº¡n 3: Giáº£m kÃ­ch thÆ°á»›c vÃ  tÄƒng chiá»u sÃ¢u ---
    print("ğŸ”§ Giai Ä‘oáº¡n 3: Giáº£m kÃ­ch thÆ°á»›c vÃ  tÄƒng Ä‘á»™ sÃ¢u...")
    # Giáº£m kÃ­ch thÆ°á»›c khÃ´ng gian vÃ  quang phá»•, Ä‘á»“ng thá»i tÄƒng sá»‘ bá»™ lá»c
    x = Conv3D(filters=64, kernel_size=(1, 1, 1), strides=(1, 1, 2), name='transition_conv')(x)
    x = BatchNormalization(name='transition_bn')(x)
    x = ReLU(name='transition_relu')(x)
    x = MaxPooling3D(pool_size=(2, 2, 2), name='transition_pool')(x)
    print(f"   Sau transition: {x.shape}")
    
    # Chá»“ng 2 khá»‘i dÆ° vá»›i 64 bá»™ lá»c
    x = residual_block(x, filters=64)
    x = residual_block(x, filters=64)
    print(f"   Sau cÃ¡c khá»‘i residual: {x.shape}")
    
    # --- Giai Ä‘oáº¡n 4: PhÃ¢n loáº¡i ---
    print("ğŸ”§ Giai Ä‘oáº¡n 4: Lá»›p phÃ¢n loáº¡i...")
    # Lá»›p Gá»™p ToÃ n cá»¥c Ä‘á»ƒ giáº£m sá»‘ lÆ°á»£ng tham sá»‘
    x = GlobalAveragePooling3D(name='global_avg_pool')(x)
    print(f"   Sau Global Average Pooling: {x.shape}")
    
    # Lá»›p Ä‘áº§u ra vá»›i hÃ m kÃ­ch hoáº¡t 'softmax' cho bÃ i toÃ¡n Ä‘a lá»›p
    outputs = Dense(num_classes, activation='softmax', name='classification_output')(x)
    print(f"   Output shape: {outputs.shape}")
        
    model = Model(inputs=inputs, outputs=outputs, name="3D_ResNet_Eggplant_N2")
    
    print("âœ… XÃ¢y dá»±ng mÃ´ hÃ¬nh hoÃ n thÃ nh!")
    return model

def compile_and_setup_callbacks(model):
    """
    5.4. BiÃªn dá»‹ch mÃ´ hÃ¬nh vÃ  thiáº¿t láº­p Callbacks
    """
    print("\n" + "=" * 60)
    print("âš™ï¸ BÆ¯á»šC 5.4: BIÃŠN Dá»ŠCH MÃ” HÃŒNH VÃ€ THIáº¾T Láº¬P CALLBACKS")
    print("=" * 60)
    
    # 1. BiÃªn dá»‹ch mÃ´ hÃ¬nh
    print("ğŸ”§ BiÃªn dá»‹ch mÃ´ hÃ¬nh...")
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='categorical_crossentropy', 
        metrics=['accuracy']
    )
    print("âœ… BiÃªn dá»‹ch hoÃ n thÃ nh!")
    
    # 2. Thiáº¿t láº­p Callbacks
    print("ğŸ“‹ Thiáº¿t láº­p Callbacks...")
    
    # Táº¡o thÆ° má»¥c Ä‘á»ƒ lÆ°u model
    if not os.path.exists('models'):
        os.makedirs('models')
        
    # ModelCheckpoint: LÆ°u láº¡i model cÃ³ val_accuracy tá»‘t nháº¥t
    checkpoint = ModelCheckpoint(
        'models/best_3d_resnet_model.keras', 
        monitor='val_accuracy', 
        save_best_only=True, 
        mode='max',
        verbose=1,
        save_format='keras'
    )
    
    # EarlyStopping: Dá»«ng sá»›m náº¿u val_loss khÃ´ng cáº£i thiá»‡n
    early_stopping = EarlyStopping(
        monitor='val_loss', 
        patience=15,  # TÄƒng patience cho deep learning
        restore_best_weights=True,
        verbose=1
    )
    
    # ReduceLROnPlateau: Giáº£m learning rate khi val_loss khÃ´ng cáº£i thiá»‡n
    reduce_lr = ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=10,
        min_lr=1e-7,
        verbose=1
    )
    
    callbacks = [checkpoint, early_stopping, reduce_lr]
    
    print("âœ… Callbacks Ä‘Ã£ Ä‘Æ°á»£c thiáº¿t láº­p:")
    print("   ğŸ“‚ ModelCheckpoint: LÆ°u model tá»‘t nháº¥t")
    print("   â¹ï¸ EarlyStopping: Dá»«ng sá»›m sau 15 epochs khÃ´ng cáº£i thiá»‡n")
    print("   ğŸ“‰ ReduceLROnPlateau: Giáº£m learning rate khi cáº§n")
    
    return callbacks

def train_model(model, X_train, y_train_cat, X_val, y_val_cat, callbacks):
    """
    5.5. Huáº¥n luyá»‡n mÃ´ hÃ¬nh
    """
    print("\n" + "=" * 60)
    print("ğŸš€ BÆ¯á»šC 5.5: HUáº¤N LUYá»†N MÃ” HÃŒNH")
    print("=" * 60)
    
    # Cáº¥u hÃ¬nh training
    EPOCHS = 100
    BATCH_SIZE = 64  # Giáº£m batch size Ä‘á»ƒ fit vá»›i GPU memory
    
    print(f"ğŸ“Š Cáº¥u hÃ¬nh huáº¥n luyá»‡n:")
    print(f"   ğŸ”¢ Epochs: {EPOCHS}")
    print(f"   ğŸ“¦ Batch size: {BATCH_SIZE}")
    print(f"   ğŸ‹ï¸ Training samples: {len(X_train):,}")
    print(f"   âœ… Validation samples: {len(X_val):,}")
    
    # ThÃªm chiá»u channel cho Conv3D input
    print("\nğŸ”„ Chuáº©n bá»‹ dá»¯ liá»‡u Ä‘áº§u vÃ o...")
    X_train_reshaped = np.expand_dims(X_train, axis=-1)  # (N, 9, 9, 277, 1)
    X_val_reshaped = np.expand_dims(X_val, axis=-1)      # (N, 9, 9, 277, 1)
    
    print(f"ğŸ“Š Input shapes sau khi reshape:")
    print(f"   X_train: {X_train_reshaped.shape}")
    print(f"   X_val: {X_val_reshaped.shape}")
    
    print("\nğŸš€ Báº¯t Ä‘áº§u huáº¥n luyá»‡n...")
    start_time = time.time()
    
    try:
        # Huáº¥n luyá»‡n mÃ´ hÃ¬nh
        history = model.fit(
            X_train_reshaped, y_train_cat,
            validation_data=(X_val_reshaped, y_val_cat),
            epochs=EPOCHS,
            batch_size=BATCH_SIZE,
            callbacks=callbacks,
            verbose=1
        )
        
        training_time = time.time() - start_time
        print(f"\nğŸ‰ Huáº¥n luyá»‡n hoÃ n táº¥t!")
        print(f"â±ï¸ Thá»i gian huáº¥n luyá»‡n: {training_time/60:.1f} phÃºt")
        print(f"ğŸ“ˆ Sá»‘ epochs Ä‘Ã£ cháº¡y: {len(history.history['loss'])}")
        
        # Hiá»ƒn thá»‹ káº¿t quáº£ cuá»‘i cÃ¹ng
        final_train_acc = history.history['accuracy'][-1]
        final_val_acc = history.history['val_accuracy'][-1]
        final_train_loss = history.history['loss'][-1]
        final_val_loss = history.history['val_loss'][-1]
        
        print(f"\nğŸ“Š Káº¾T QUáº¢ CUá»I CÃ™NG:")
        print(f"   ğŸ‹ï¸ Training Accuracy: {final_train_acc:.4f}")
        print(f"   âœ… Validation Accuracy: {final_val_acc:.4f}")
        print(f"   ğŸ“‰ Training Loss: {final_train_loss:.4f}")
        print(f"   ğŸ“‰ Validation Loss: {final_val_loss:.4f}")
        
        return history
        
    except Exception as e:
        print(f"âŒ Lá»—i trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n: {str(e)}")
        return None

def visualize_training_results(history):
    """
    5.6. Trá»±c quan hÃ³a Káº¿t quáº£ Huáº¥n luyá»‡n
    """
    print("\n" + "=" * 60)
    print("ğŸ“Š BÆ¯á»šC 5.6: TRá»°C QUAN HÃ“A Káº¾T QUáº¢")
    print("=" * 60)
    
    if history is None:
        print("âŒ KhÃ´ng cÃ³ lá»‹ch sá»­ huáº¥n luyá»‡n Ä‘á»ƒ trá»±c quan hÃ³a")
        return
    
    # Táº¡o figure vá»›i 2 subplots
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # Äá»“ thá»‹ Accuracy
    ax1.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)
    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)
    ax1.set_title('Model Accuracy Over Time', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Accuracy')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # ThÃªm annotation cho accuracy cao nháº¥t
    max_val_acc = max(history.history['val_accuracy'])
    max_val_epoch = history.history['val_accuracy'].index(max_val_acc)
    ax1.annotate(f'Best Val Acc: {max_val_acc:.4f}',
                xy=(max_val_epoch, max_val_acc),
                xytext=(max_val_epoch+5, max_val_acc-0.02),
                arrowprops=dict(arrowstyle='->', color='red'),
                fontsize=10, color='red')
    
    # Äá»“ thá»‹ Loss
    ax2.plot(history.history['loss'], label='Training Loss', linewidth=2)
    ax2.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)
    ax2.set_title('Model Loss Over Time', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Loss')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # ThÃªm annotation cho loss tháº¥p nháº¥t
    min_val_loss = min(history.history['val_loss'])
    min_val_epoch = history.history['val_loss'].index(min_val_loss)
    ax2.annotate(f'Best Val Loss: {min_val_loss:.4f}',
                xy=(min_val_epoch, min_val_loss),
                xytext=(min_val_epoch+5, min_val_loss+0.05),
                arrowprops=dict(arrowstyle='->', color='red'),
                fontsize=10, color='red')
    
    plt.tight_layout()
    
    # LÆ°u biá»ƒu Ä‘á»“
    plt.savefig('models/training_history.png', dpi=300, bbox_inches='tight')
    print("ğŸ“Š Biá»ƒu Ä‘á»“ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u: models/training_history.png")
    
    plt.show()
    
    # In thá»‘ng kÃª chi tiáº¿t
    print("\nğŸ“ˆ THá»NG KÃŠ CHI TIáº¾T:")
    print(f"   ğŸ¯ Best Validation Accuracy: {max_val_acc:.4f} (Epoch {max_val_epoch+1})")
    print(f"   ğŸ“‰ Best Validation Loss: {min_val_loss:.4f} (Epoch {min_val_epoch+1})")
    print(f"   ğŸ“Š Total Epochs: {len(history.history['loss'])}")
    
    # Kiá»ƒm tra overfitting
    final_train_acc = history.history['accuracy'][-1]
    final_val_acc = history.history['val_accuracy'][-1]
    acc_gap = final_train_acc - final_val_acc
    
    if acc_gap > 0.1:
        print(f"âš ï¸ CÃ³ dáº¥u hiá»‡u overfitting (Train-Val gap: {acc_gap:.4f})")
    else:
        print(f"âœ… MÃ´ hÃ¬nh á»•n Ä‘á»‹nh (Train-Val gap: {acc_gap:.4f})")

def main():
    """
    HÃ m chÃ­nh thá»±c hiá»‡n toÃ n bá»™ quy trÃ¬nh BÆ°á»›c 5
    """
    start_time = time.time()
    
    # BÆ°á»›c 5.1: Táº£i dá»¯ liá»‡u
    X_train, y_train, X_val, y_val, metadata = load_processed_data()
    if X_train is None:
        return
    
    # BÆ°á»›c 5.1: Chuáº©n bá»‹ nhÃ£n
    NUM_CLASSES = metadata['n_classes']
    y_train_cat, y_val_cat = prepare_labels_for_training(y_train, y_val, NUM_CLASSES)
    
    # BÆ°á»›c 5.2 & 5.3: XÃ¢y dá»±ng mÃ´ hÃ¬nh
    INPUT_SHAPE = (metadata['patch_size'], metadata['patch_size'], metadata['n_bands'])
    model = build_detailed_3d_resnet(INPUT_SHAPE, NUM_CLASSES)
    
    # Hiá»ƒn thá»‹ thÃ´ng tin mÃ´ hÃ¬nh
    print(f"\nğŸ“‹ THÃ”NG TIN MÃ” HÃŒNH:")
    model.summary()
    
    # BÆ°á»›c 5.4: BiÃªn dá»‹ch vÃ  thiáº¿t láº­p callbacks
    callbacks = compile_and_setup_callbacks(model)
    
    # BÆ°á»›c 5.5: Huáº¥n luyá»‡n
    history = train_model(model, X_train, y_train_cat, X_val, y_val_cat, callbacks)
    
    # BÆ°á»›c 5.6: Trá»±c quan hÃ³a káº¿t quáº£
    visualize_training_results(history)
    
    # Tá»•ng káº¿t
    total_time = time.time() - start_time
    print("\n" + "=" * 60)
    print("ğŸ‰ HOÃ€N THÃ€NH BÆ¯á»šC 5")
    print("=" * 60)
    print("âœ… MÃ´ hÃ¬nh 3D-ResNet Ä‘Ã£ Ä‘Æ°á»£c xÃ¢y dá»±ng")
    print("âœ… Huáº¥n luyá»‡n hoÃ n táº¥t")
    print("âœ… Káº¿t quáº£ Ä‘Ã£ Ä‘Æ°á»£c trá»±c quan hÃ³a")
    print("âœ… MÃ´ hÃ¬nh tá»‘t nháº¥t Ä‘Ã£ Ä‘Æ°á»£c lÆ°u")
    print(f"â±ï¸ Tá»•ng thá»i gian: {total_time/60:.1f} phÃºt")
    print("â¡ï¸ Sáºµn sÃ ng cho BÆ°á»›c 6: ÄÃ¡nh giÃ¡ vÃ  Test")
    print("=" * 60)

if __name__ == "__main__":
    main()
